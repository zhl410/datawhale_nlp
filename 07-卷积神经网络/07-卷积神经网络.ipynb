{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "卷积运算的定义、动机（稀疏权重、参数共享、等变表示）。\n",
    "\n",
    "一维卷积运算和二维卷积运算。\n",
    "\n",
    "池化运算的定义、种类（最大池化、平均池化等）、动机。\n",
    "\n",
    "Text-CNN的原理。\n",
    "\n",
    "利用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积神经网络 ##\n",
    "卷积网络(LeCun, 1989)，又称卷积神经网络\n",
    "CNNs是一种特殊的神经网络，用于处理具有已知网格状拓扑结构的数据。例如，时间序列数据可以看作是一个按一定时间间隔采样的一维网格，图像数据可以看作是一个由像素组成的二维网格"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卷积 ##\n",
    "“卷积神经网络”的名称表明，该网络采用了一种称为卷积的数学运算。卷积是一种特殊的线性运算。卷积网络是一种简单的神经网络，它在至少一层中使用卷积来代替一般的矩阵乘法。\n",
    "\n",
    "$$s(t) = \\int x(a)w(t-a)da $$\n",
    "$$s(t) = (x*w)(t)$$\n",
    "在卷积网络术语中，卷积的第一个参数(在本例中是函数x)通常被称为输入(input)，第二个参数(在本例中是函数w)作为内核(kernel)。输出有时称为特征图(feature map)。\n",
    "\n",
    "如果我们现在假设x和w只在整数t上定义，我们可以定义离散卷积:\n",
    "    $$s(t) = (x*w)(t) = \\sum_{a=-\\infty }^{\\infty}x(a)w(t-a)$$\n",
    "    \n",
    "如果我们使用二维图像I作为输入，我们可以使用二维卷积核K:\n",
    "\n",
    "$$ S(i, j) = (I*K)(i,j)=\\sum_m\\sum_m I(m, n)K(i-m, j-n)$$\n",
    "\n",
    "时间上许多神经网络库实现的卷积函数并不是以上数学定义上的卷积，而是叫做cross-correlation的函数，它和convolution有相同的形式，但是没有对Kernel进行翻转:\n",
    "$$S(i, j)=(K * I)(i,j)=\\sum_m\\sum_n I(i+m, j+n)K(m, n)$$ \n",
    "![二维卷积图](二维卷积图.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 稀疏权重 ##\n",
    "稀疏连接是通过使内核小于输入来实现的。\n",
    "如果有m个输入和n个输出，那么矩阵乘法需要m×n个参数，而实际使用的算法运行时为O(m×n)(每个例子)。如果我们将每个输出的连接数限制为k，那么稀疏连接方法只需要k×n个参数和O(k×n)运行时。在许多实际应用中，在保持k比m小几个数量级的情况下，仍有可能获得良好的机器学习性能。\n",
    "\n",
    "![稀疏连接](稀疏连接.png)\n",
    "\n",
    "稀疏连接，从下往上面看。我们突出显示一个输入单元x3，并突出显示受该单元影响的s中的输出单元。(上)当s与宽度为3的核卷积形成时，只有3个输出受到x的影响。(下)当s由矩阵乘法形成时，连通性不再稀疏，所以所有的输出都受到x3的影响。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参数共享 ##\n",
    "参数共享是指对模型中的多个函数使用相同的参数。在传统的神经网络中，每个元素的权值矩阵在计算层的输出时仅使用一次。它乘以输入的一个元素，然后就再也不会被访问了。作为参数共享的同义词，可以说网络具有绑定的权重，因为应用于一个输入的权重值与应用于其他地方的权重值绑定在一起。在卷积神经网络中，在输入的每个位置都使用内核的每个成员(除了一些边界像素，这取决于关于边界的设计决策)。卷积运算中使用的参数共享意味着不必为每个位置学习一组单独的参数。\n",
    "![参数共享](参数共享.png)\n",
    "\n",
    "黑色箭头表示在两个不同模型中使用特定参数的连接。(顶部)黑色箭头表示在卷积模型中使用3元素内核的中心元素。由于参数共享，此单个参数用于所有输入位置。(下)单个黑色箭头表示在完全连通模型中使用权重矩阵的中心元素。该模型没有参数共享，因此该参数只使用一次。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 等变表示 ##\n",
    "卷积的情况下，参数共享的特殊形式导致层具有一个称为等变表示的属性。说函数是等变的意思是如果输入改变了，输出也会以同样的方式改变。\n",
    "具体地说，如果f(g(x)) = g(f(x))，函数f(x)与函数g是等价的。在卷积的情况下，如果我们让g是任何一个函数，它平移输入，那么卷积函数与g是等变的。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 池化 ##\n",
    "卷积网络的典型层由三个阶段组成。在第一阶段，该层并行执行几个卷积，以生成一组线性激活。在第二阶段，每一个线性激活都经过一个非线性激活函数，如校正后的线性激活函数。这个阶段有时被称为检测器阶段。在第三阶段，我们使用池函数进一步修改层的输出。\n",
    "![标准卷积块](标准卷积块.png)\n",
    "max pooling\n",
    "![max_pooling](max_pooling.png)\n",
    "average pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 动机 ##\n",
    "在所有情况下，池化都有助于使表示形式对输入的小平移保持近似不变。平移的不变性意味着，如果我们对输入进行少量平移，那么大多数合并输出的值都不会改变。如果我们更关心某个特性是否存在，而不是它的确切位置，那么局部翻译的不变性可能是一个有用的属性。\n",
    "池的使用可以看作是添加了一个无限强的先验，即层所学习的函数必须对小的转换保持不变。当这个假设是正确的，它可以大大提高网络的统计效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text-CNN ##\n",
    "\n",
    "将卷积神经网络CNN应用到文本分类任务，利用多个不同size的kernel来提取句子中的关键信息（类似于多窗口大小的ngram），从而能够更好地捕捉局部相关性\n",
    "![textcnn架构](textcnn架构.png)\n",
    "![textcnn](textcnn.png)\n",
    "两个通道的模型架构"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env01(py36)",
   "language": "python",
   "name": "env01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
